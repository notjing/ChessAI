import tensorflow as tf
from huggingface_hub import HfApi
import os

def get_dataset(files, batch_size):
    return (
        files
        # allows for several tfrecords to be read at a time
        .interleave(tf.data.TFRecordDataset, num_parallel_calls=tf.data.AUTOTUNE)
        # applies parse_tfrecord onto all files
        .map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)
        .shuffle(200_000)
        .batch(batch_size, drop_remainder=True)
        .repeat()
        # allows CPU to fetch more batches while GPU is calculating
        .prefetch(tf.data.AUTOTUNE)
    )

def pawn_error(y_true, y_pred):
    """
    Converts the output into delta centipawns
    """
    return tf.reduce_mean(abs(y_true - y_pred)) * 1500


def parse_tfrecord(example):
    """
    Unwraps all the TFRecord files
    """

    # Provides the structure of the TFRecord data
    feature_desc = {
        "board": tf.io.FixedLenFeature([8 * 8 * 15], tf.float32),
        "extra": tf.io.FixedLenFeature([19], tf.float32),
        "eval": tf.io.FixedLenFeature([1], tf.float32),
    }

    ex = tf.io.parse_single_example(example, feature_desc)

    # Adjusts eval
    clamped_eval = tf.clip_by_value(ex["eval"][0], -1500.0, 1500.0)
    evalv = clamped_eval / 1500.0

    board = tf.reshape(ex["board"], (8, 8, 15))
    return {"board_input": board, "extra_input": ex["extra"]}, evalv
def main():
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    TFRECORD_DIR = os.path.join(BASE_DIR, "tfrecords")

    all_files = tf.data.Dataset.list_files(os.path.join(TFRECORD_DIR, "*.tfrecord"), shuffle=True)
    val_size = 10

    test_files = all_files.take(val_size)
    train_files = all_files.skip(val_size)

    train_ds = get_dataset(train_files, 512)
    test_ds = get_dataset(test_files, 512)


    # Define CNN + dense model
    cnn_input = tf.keras.Input(shape=(8, 8, 15), name="board_input")
    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding="same")(cnn_input)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding="same")(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding="same", kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(256, activation='relu')(x)

    extra_input = tf.keras.Input(shape=(19,), name="extra_input")
    y = tf.keras.layers.Dense(32, activation='relu')(extra_input)
    y = tf.keras.layers.Dense(16, activation='relu')(y)

    combined = tf.keras.layers.Concatenate()([x, y])
    z = tf.keras.layers.Dense(64, activation='relu')(combined)
    z = tf.keras.layers.Dense(32, activation='relu')(z)
    output = tf.keras.layers.Dense(1, activation='linear')(z)

    aimodel = tf.keras.Model(inputs=[cnn_input, extra_input], outputs=output)
    aimodel.compile(optimizer=tf.keras.optimizers.Adam(0.0002),
                     loss=tf.keras.losses.MeanSquaredError(),
                     metrics=['mse', pawn_error])

    # Train the model
    print("Starting training...")
    aimodel.fit(
        train_ds,
        validation_data=test_ds,
        epochs=25,
        steps_per_epoch=1000,
    )

    print("Evaluating...")
    aimodel.evaluate(test_ds, steps=100, verbose=2)

    model_file = "chessai_model.keras"
    aimodel.save(model_file)

    print("Uploading to Hugging Face...")
    api = HfApi()

    api.upload_file(
        path_or_fileobj=model_file,
        path_in_repo="chessai_model.keras",
        repo_id="notjing/chessai",
        repo_type="model",
        commit_message="Upload trained chess AI model"
    )
    print("Model uploaded successfully!")


if __name__ == "__main__":
    main()
